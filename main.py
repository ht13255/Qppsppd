import streamlit as st
import arxiv
import fitz  # PyMuPDF
import spacy
import json
import os
import re
import time
from datetime import datetime, timedelta

# --- ìƒìˆ˜ ë° ì„¤ì • ---
STORAGE_PATH = "./papers"
OUTPUT_FILENAME = "knowledge_graph_live.json"
OUTPUT_PATH = os.path.join(STORAGE_PATH, OUTPUT_FILENAME)

# --- í•µì‹¬ ë¡œì§ í´ë˜ìŠ¤ ---
class KnowledgeGraphGenerator:
    """ArXiv ë…¼ë¬¸ì„ ì²˜ë¦¬í•˜ì—¬ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” ë°±ì—”ë“œ ë¡œì§."""
    
    @st.cache_resource
    def load_spacy_model(_self):
        """Streamlitì˜ ìºì‹œë¥¼ ì‚¬ìš©í•´ spaCy ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
        with st.spinner("ì–¸ì–´ ëª¨ë¸(spaCy)ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤..."):
            try
                model = spacy.load("en_core_web_sm")
                return model
            except OSError:
                st.error("spaCy ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í„°ë¯¸ë„ì—ì„œ 'python -m spacy download en_core_web_sm'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
                st.stop()

    def __init__(self):
        self.nlp = self.load_spacy_model()

    def search_papers(self, keyword, max_results):
        """ArXivì—ì„œ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            search = arxiv.Search(
                query=keyword,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.Relevance
            )
            return list(search.results())
        except Exception as e:
            st.error(f"ArXiv API ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return []

    def process_single_paper(self, paper_result):
        """ë‹¨ì¼ ë…¼ë¬¸ì„ ë‹¤ìš´ë¡œë“œ, ë¶„ì„í•˜ê³  íŠ¸ë¦¬í”Œì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            safe_title = re.sub(r'[\\/*?:"<>|]', "", paper_result.title)
            pdf_path = os.path.join(STORAGE_PATH, f"{safe_title}.pdf")
            if not os.path.exists(pdf_path):
                paper_result.download_pdf(dirpath=STORAGE_PATH, filename=f"{safe_title}.pdf")
            
            paper_info = {"title": paper_result.title, "url": paper_result.entry_id}
            text = self.extract_text_from_pdf(pdf_path)
            if not text:
                return []

            return self.create_triples_from_text(text, paper_info)
        except Exception:
            return [] # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    def extract_text_from_pdf(self, pdf_path):
        try:
            with fitz.open(pdf_path) as doc:
                text = " ".join(page.get_text() for page in doc)
            return re.sub(r'\s+', ' ', text).replace('-\n', '')
        except Exception:
            return ""

    def create_triples_from_text(self, text, paper_info, chunk_size=50000):
        triples = []
        # NLP ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ í•œê³„ë¥¼ ê³ ë ¤í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(text), chunk_size):
            chunk = text[i:i + chunk_size]
            doc = self.nlp(chunk)
            for sent in doc.sents:
                subjects = [tok for tok in sent if "subj" in tok.dep_]
                objects = [tok for tok in sent if "obj" in tok.dep_]
                if subjects and objects:
                    verb = sent.root
                    for sub in subjects:
                        for obj in objects:
                            subject_phrase = " ".join(t.text for t in sub.subtree)
                            object_phrase = " ".join(t.text for t in obj.subtree)
                            triples.append({
                                "subject": subject_phrase,
                                "predicate": verb.lemma_,
                                "object": object_phrase,
                                "source_title": paper_info['title'],
                                "source_url": paper_info['url']
                            })
        return triples

    def append_triples_to_json(self, new_triples):
        """ê¸°ì¡´ JSON íŒŒì¼ì— ìƒˆë¡œìš´ íŠ¸ë¦¬í”Œ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        if not new_triples:
            return
        
        existing_data = []
        if os.path.exists(OUTPUT_PATH):
            try:
                with open(OUTPUT_PATH, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except json.JSONDecodeError:
                pass # íŒŒì¼ì´ ë¹„ì–´ ìˆìœ¼ë©´ ë¬´ì‹œ
        
        existing_data.extend(new_triples)
        
        with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=4)

# --- Streamlit UI ---
def main():
    st.set_page_config(page_title="ArXiv ëŒ€ê·œëª¨ ë¶„ì„ê¸°", layout="wide")
    st.title("ğŸš€ ArXiv ëŒ€ê·œëª¨ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±ê¸°")
    st.markdown("APIê°€ í—ˆìš©í•˜ëŠ” ìµœëŒ€ ìˆ˜ì¤€ê¹Œì§€ ë…¼ë¬¸ì„ ë¶„ì„í•˜ì—¬ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

    with st.sidebar:
        st.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
        keyword = st.text_input("ê²€ìƒ‰ í‚¤ì›Œë“œ", "transformer model")
        max_papers = st.number_input("ìµœëŒ€ ë…¼ë¬¸ ìˆ˜", min_value=1, max_value=30000, value=100)
        
        st.warning(
            "âš ï¸ **ì£¼ì˜:** 100ê°œ ì´ìƒì˜ ë…¼ë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ê²ƒì€ ë§¤ìš° ê¸´ ì‹œê°„(ìˆ˜ì‹­ ë¶„ ~ ìˆ˜ ì‹œê°„)ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
            "ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        )
        
        start_button = st.button("ëŒ€ê·œëª¨ ë¶„ì„ ì‹œì‘", type="primary")

    if start_button:
        # í´ë” ë° ê²°ê³¼ íŒŒì¼ ì´ˆê¸°í™”
        os.makedirs(STORAGE_PATH, exist_ok=True)
        if os.path.exists(OUTPUT_PATH):
            os.remove(OUTPUT_PATH)

        generator = KnowledgeGraphGenerator()
        
        with st.status("ArXivì—ì„œ ë…¼ë¬¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...", expanded=True) as status:
            papers = generator.search_papers(keyword, max_papers)
            if not papers:
                status.update(label="ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ", state="error")
                st.error("í•´ë‹¹ í‚¤ì›Œë“œì˜ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()
            
            status.update(label=f"ì´ {len(papers)}ê°œì˜ ë…¼ë¬¸ ë°œê²¬! ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.", state="running")

        st.info(f"'{keyword}'ì— ëŒ€í•´ ì´ {len(papers)}ê°œì˜ ë…¼ë¬¸ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤. ëª¨ë“  ê²°ê³¼ëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.")
        
        # ì§„í–‰ ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ UI ìš”ì†Œ
        progress_bar = st.progress(0)
        progress_text = st.empty()
        
        start_time = time.time()
        total_triples_found = 0

        for i, paper in enumerate(papers):
            # ETA ê³„ì‚°
            elapsed_time = time.time() - start_time
            papers_processed = i + 1
            avg_time_per_paper = elapsed_time / papers_processed
            remaining_papers = len(papers) - papers_processed
            eta_seconds = remaining_papers * avg_time_per_paper
            eta_str = str(timedelta(seconds=int(eta_seconds)))

            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            progress_bar.progress(papers_processed / len(papers))
            progress_text.markdown(
                f"**ì§„í–‰ë¥ : {papers_processed}/{len(papers)}** | "
                f"**ì°¾ì€ íŠ¸ë¦¬í”Œ: {total_triples_found}ê°œ** | "
                f"**ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {eta_str}**"
                f"<br>ğŸ“„ í˜„ì¬ ë¶„ì„ ì¤‘: *{paper.title[:80]}...*",
                unsafe_allow_html=True
            )

            # í•µì‹¬ ë¡œì§ ì‹¤í–‰
            triples = generator.process_single_paper(paper)
            if triples:
                total_triples_found += len(triples)
                generator.append_triples_to_json(triples)
        
        progress_bar.progress(1.0)
        progress_text.success(f"âœ… ë¶„ì„ ì™„ë£Œ! ì´ {total_triples_found}ê°œì˜ íŠ¸ë¦¬í”Œì„ ì°¾ì•„ '{OUTPUT_PATH}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
        # ì„¸ì…˜ ìƒíƒœì— ìµœì¢… ê²°ê³¼ ë¡œë“œ
        if os.path.exists(OUTPUT_PATH):
            with open(OUTPUT_PATH, 'r', encoding='utf-8') as f:
                st.session_state['results'] = json.load(f)

    # ë¶„ì„ì´ ì™„ë£Œë˜ê±°ë‚˜ ê¸°ì¡´ ê²°ê³¼ê°€ ìˆì„ ë•Œ í‘œì‹œ
    if 'results' in st.session_state and st.session_state['results']:
        st.header("ğŸ“Š ìµœì¢… ë¶„ì„ ê²°ê³¼")
        
        results_data = st.session_state['results']
        json_string = json.dumps(results_data, indent=4, ensure_ascii=False)
        
        st.download_button(
            label=f"ğŸ“„ JSON íŒŒì¼ ë‹¤ìš´ë¡œë“œ ({len(results_data)}ê°œ íŠ¸ë¦¬í”Œ)",
            data=json_string,
            file_name=f"kg_{keyword.replace(' ', '_')}.json",
            mime="application/json",
        )

        st.dataframe(results_data, height=600)

if __name__ == "__main__":
    main()
